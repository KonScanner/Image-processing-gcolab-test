{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image-Processing-caffe-cw.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNI13AC+yKCpyDvQVjcsAB1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KonScanner/Image-processing-gcolab-test/blob/master/Image_Processing_caffe_cw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSxvM9a81AEC",
        "colab_type": "text"
      },
      "source": [
        "# Preparing Caffe for Google-colab\n",
        "\n",
        "- If cuda installed >= 10.0 follow the rest of the steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDidOsk01RpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "!nvidia-smi\n",
        "!nvcc -V\n",
        "\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCMGGd4u1dzz",
        "colab_type": "text"
      },
      "source": [
        "## Build Caffe\n",
        "- Dependencies needed:\n",
        "  - opencv, OpenBLAS, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYcW3eYz1SKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt install python3-opencv\n",
        "!apt-get install libopenblas-dev\n",
        "!apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler\n",
        "!apt-get install â€” no-install-recommends libboost-all-dev\n",
        "!apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev\n",
        "!pip3 install protobuf\n",
        "!apt-get install the python3-dev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5rWazl81t71",
        "colab_type": "text"
      },
      "source": [
        "### Clone git Caffe project and build\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GGCy_NK1tm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content')\n",
        "!rm -rf caffe\n",
        "!git clone https://github.com/TimeIsTheChoice/caffe.git\n",
        "!cd caffe\n",
        "!nproc --all\n",
        "!echo \"Threads/core: $(nproc --all)\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGLv0HVT1_sO",
        "colab_type": "text"
      },
      "source": [
        "### Building pycafe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmkjRU_U1q4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/caffe')\n",
        "!make all -j$(nproc --all)\n",
        "!make pycaffe -j$(nproc --all)\n",
        "!export PYTHONPATH=~/caffe/python:$PYTHONPATH"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9zCs3Kt2RU5",
        "colab_type": "text"
      },
      "source": [
        "### Import Caffe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71-P7QC12M1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "caffe_root = '/content/caffe/'\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, caffe_root + 'python')\n",
        "import caffe\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBveqMoQ2Tuw",
        "colab_type": "text"
      },
      "source": [
        "### GPU environment information\n",
        "- Check GPU memory utilization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJCNT6If2YN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "\n",
        "# only one GPU on Colab\n",
        "gpu = GPUs[0]\n",
        "def gpu_util():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "gpu_util() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1MHZFtE2yYw",
        "colab_type": "text"
      },
      "source": [
        "## Solving wih LeNet\n",
        "- Mnist exmaple"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtyFFVrM2Ywx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pylab import *\n",
        "%matplotlib inline\n",
        "\n",
        "caffe_root='/content/caffe/'\n",
        "import sys\n",
        "sys.path.insert(0, caffe_root + 'python')\n",
        "import caffe\n",
        "\n",
        "# run scripts from caffe root\n",
        "import os\n",
        "os.chdir(caffe_root)\n",
        "# Download data\n",
        "!data/mnist/get_mnist.sh\n",
        "# Prepare data\n",
        "!examples/mnist/create_mnist.sh\n",
        "# back to examples\n",
        "os.chdir('examples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQcWZZbr3H8u",
        "colab_type": "text"
      },
      "source": [
        "### Building the network \n",
        "\n",
        "Now let's make a variant of LeNet, the classic 1989 convnet architecture.\n",
        "\n",
        "We'll need two external files to help out:\n",
        "* the net `prototxt`, defining the architecture and pointing to the train/test data\n",
        "* the solver `prototxt`, defining the learning parameters\n",
        "\n",
        "We start by creating the net. We'll write the net in a succinct and natural way as Python code that serializes to Caffe's protobuf model format.\n",
        "\n",
        "This network expects to read from pregenerated LMDBs, but reading directly from `ndarray`s is also possible using `MemoryDataLayer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMc8l_3n3N4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from caffe import layers as L, params as P\n",
        "\n",
        "def lenet(lmdb, batch_size):\n",
        "    # our version of LeNet: a series of linear and simple nonlinear transformations\n",
        "    n = caffe.NetSpec()\n",
        "    \n",
        "    n.data, n.label = L.Data(batch_size=batch_size, backend=P.Data.LMDB, source=lmdb,\n",
        "                             transform_param=dict(scale=1./255), ntop=2)\n",
        "    \n",
        "    n.conv1 = L.Convolution(n.data, kernel_size=5, num_output=20, weight_filler=dict(type='xavier'))\n",
        "    n.pool1 = L.Pooling(n.conv1, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
        "    n.conv2 = L.Convolution(n.pool1, kernel_size=5, num_output=50, weight_filler=dict(type='xavier'))\n",
        "    n.pool2 = L.Pooling(n.conv2, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
        "    n.fc1 =   L.InnerProduct(n.pool2, num_output=500, weight_filler=dict(type='xavier'))\n",
        "    n.relu1 = L.ReLU(n.fc1, in_place=True)\n",
        "    n.score = L.InnerProduct(n.relu1, num_output=10, weight_filler=dict(type='xavier'))\n",
        "    n.loss =  L.SoftmaxWithLoss(n.score, n.label)\n",
        "    \n",
        "    return n.to_proto()\n",
        "    \n",
        "with open('mnist/lenet_auto_train.prototxt', 'w') as f:\n",
        "    f.write(str(lenet('mnist/mnist_train_lmdb', 64)))\n",
        "    \n",
        "with open('mnist/lenet_auto_test.prototxt', 'w') as f:\n",
        "    f.write(str(lenet('mnist/mnist_test_lmdb', 100)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf54FReE3Sx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pwd\n",
        "!cat mnist/lenet_auto_train.prototxt\n",
        "!cat mnist/lenet_auto_solver.prototxt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrNiQhMy3e-A",
        "colab_type": "text"
      },
      "source": [
        "### Loading and checking the solver\n",
        "- Pick a device and load the solver. We'll use SGD (with momentum), but other methods (such as Adagrad and Nesterov's accelerated gradient) are also available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaukxWit3U9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "caffe.set_device(0)\n",
        "caffe.set_mode_gpu()\n",
        "!chmod 777 /content/caffe/* -R\n",
        "solver = caffe.SGDSolver('/content/caffe/examples/mnist/lenet_auto_solver.prototxt')\n",
        "\n",
        "# each output is (batch size, feature dim, spatial dim)\n",
        "[(k, v.data.shape) for k, v in solver.net.blobs.items()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zXhjUJ13rnb",
        "colab_type": "text"
      },
      "source": [
        "- Before taking off, let's check that everything is loaded as we expect. We'll run a forward pass on the train and test nets and check that they contain our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDqc0qDM3sfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%timeit solver.net.forward()  # train net\n",
        "solver.net.forward()  # train net\n",
        "%timeit solver.test_nets[0].forward()  # test net (there can be more than one)\n",
        "solver.test_nets[0].forward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haVthBXs3uZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we use a little trick to tile the first eight images\n",
        "num = 8\n",
        "imshow(solver.net.blobs['data'].data[:num, 0].transpose(1, 0, 2).reshape(28, num*28), cmap='gray'); axis('off')\n",
        "print ('train labels:', solver.net.blobs['label'].data[:num])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B3pfTKQ3xAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Output intermediate features (blobs)\n",
        "for blob in solver.net.blobs:\n",
        "    print(blob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geg7J_BY317c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "layer_name = 'fc1'\n",
        "for layer_name in solver.net.blobs:\n",
        "  dimention = len(solver.net.blobs[layer_name].shape)\n",
        "\n",
        "  d=[]\n",
        "  for dim in range(dimention):\n",
        "    d.append(solver.net.blobs[layer_name].shape[dim])\n",
        "\n",
        "  if dimention == 4:\n",
        "    for d_1 in range(d[1]):\n",
        "      #print(solver.net.blobs[layer_name].data[:,d_1,:,:].shape)\n",
        "      num = 1\n",
        "      #num=d[0]\n",
        "      plt.figure(figsize = (num,d[2]))\n",
        "      plt.imshow(solver.net.blobs[layer_name].data[:num,d_1,:,:].transpose(1, 0, 2).reshape(d[2], num*d[3]), interpolation='nearest',cmap='gray'); axis('off')\n",
        "  elif dimention == 3:\n",
        "    print(\"-----------\")\n",
        "  elif dimention ==2:\n",
        "    #num = 1\n",
        "    num=d[0]\n",
        "    plt.figure(figsize = (num,d[1]))\n",
        "    plt.imshow(solver.net.blobs[layer_name].data[:num,:], interpolation='nearest',cmap='gray'); axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSq4A93u3_yG",
        "colab_type": "text"
      },
      "source": [
        "### Stepping the solver\n",
        "- Both `train` and `test` nets seem to be loading data, and to have correct labels. Let's take one step of (minibatch) SGD and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq_bL7hP4Fuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%timeit solver.step(2000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl6Ars8V4Mez",
        "colab_type": "text"
      },
      "source": [
        "- Do we have gradients propagating through our filters? Let's see the updates to the first layer, shown here as a $4 \\times 5$ grid of $5 \\times 5$ filters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf5nqO9a4IHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imshow(solver.net.params['conv1'][0].diff[:, 0].reshape(4, 5, 5, 5)\n",
        "       .transpose(0, 2, 1, 3).reshape(4*5, 5*5), cmap='gray'); axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YZHUnJx4TFt",
        "colab_type": "text"
      },
      "source": [
        "### Writing a custom training loop\n",
        "\n",
        "Something is happening. Let's run the net for a while, keeping track of a few things as it goes.\n",
        "Note that this process will be the same as if training through the `caffe` binary. In particular:\n",
        "* logging will continue to happen as normal\n",
        "* snapshots will be taken at the interval specified in the solver prototxt (here, every 5000 iterations)\n",
        "* testing will happen at the interval specified (here, every 500 iterations)\n",
        "\n",
        "Since we have control of the loop in Python, we're free to compute additional things as we go, as we show below. We can do many other things as well, for example:\n",
        "* write a custom stopping criterion\n",
        "* change the solving process by updating the net in the loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP1vgweW4P9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "niter = 20000 \n",
        "test_interval = 500\n",
        "# losses will also be stored in the log\n",
        "train_loss = zeros(niter)\n",
        "test_acc = zeros(int(np.ceil(niter / test_interval)))\n",
        "output = zeros((niter, 8, 10))\n",
        "\n",
        "# the main solver loop\n",
        "for it in range(niter):\n",
        "    solver.step(1)  # SGD by Caffe\n",
        "    \n",
        "    # store the train loss\n",
        "    train_loss[it] = solver.net.blobs['loss'].data\n",
        "    \n",
        "    # store the output on the first test batch\n",
        "    # (start the forward pass at conv1 to avoid loading new data)\n",
        "    solver.test_nets[0].forward(start='conv1')\n",
        "    output[it] = solver.test_nets[0].blobs['score'].data[:8]\n",
        "    \n",
        "    # run a full test every so often\n",
        "    # (Caffe can also do this for us and write to a log, but we show here\n",
        "    #  how to do it directly in Python, where more complicated things are easier.)\n",
        "    if it % test_interval == 0:\n",
        "        print('Iteration', it, 'testing...')\n",
        "        correct = 0\n",
        "        for test_it in range(100):\n",
        "            solver.test_nets[0].forward()\n",
        "            correct += sum(solver.test_nets[0].blobs['score'].data.argmax(1)\n",
        "                           == solver.test_nets[0].blobs['label'].data)\n",
        "        test_acc[it // test_interval] = correct / 1e4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63DThuXL4b2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check the accuracy\n",
        "_, ax1 = subplots()\n",
        "ax2 = ax1.twinx()\n",
        "ax1.plot(arange(niter), train_loss)\n",
        "ax2.plot(test_interval * arange(len(test_acc)), test_acc, 'r')\n",
        "ax1.set_xlabel('iteration')\n",
        "ax1.set_ylabel('train loss')\n",
        "ax2.set_ylabel('test accuracy')\n",
        "ax2.set_title('Test Accuracy: {:.2f}'.format(test_acc[-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ynw0EBPr4d_9",
        "colab_type": "text"
      },
      "source": [
        "The loss seems to have dropped quickly and coverged (except for stochasticity), while the accuracy rose correspondingly.\n",
        "\n",
        "* Since we saved the results on the first test batch, we can watch how our prediction scores evolved. We'll plot time on the $x$ axis and each possible label on the $y$, with lightness indicating confidence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQoGNiB44g-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(5):\n",
        "    figure(figsize=(2, 2))\n",
        "    imshow(solver.test_nets[0].blobs['data'].data[i, 0], cmap='gray')\n",
        "    figure(figsize=(10, 2))\n",
        "    imshow(output[:50, i].T, interpolation='nearest', cmap='gray')\n",
        "    xlabel('iteration')\n",
        "    ylabel('label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqKSNJr64lCi",
        "colab_type": "text"
      },
      "source": [
        "We started with little idea about any of these digits, and ended up with correct classifications for each. If you've been following along, you'll see the last digit is the most difficult, a slanted \"9\" that's (understandably) most confused with \"4\".\n",
        "\n",
        "* Note that these are the \"raw\" output scores rather than the softmax-computed probability vectors. The latter, shown below, make it easier to see the confidence of our net (but harder to see the scores for less likely digits)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axquuNrW4laI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(8):\n",
        "    figure(figsize=(2, 2))\n",
        "    imshow(solver.test_nets[0].blobs['data'].data[i, 0], cmap='gray')\n",
        "    figure(figsize=(10, 2))\n",
        "    imshow(exp(output[:50, i].T) / exp(output[:50, i].T).sum(0), interpolation='nearest', cmap='gray')\n",
        "    xlabel('iteration')\n",
        "    ylabel('label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydxus7R24qtl",
        "colab_type": "text"
      },
      "source": [
        "### Experiment with your own architecture and optimization\n",
        "\n",
        "Now that we've defined, trained, and tested LeNet there are many possible next steps:\n",
        "\n",
        "- Define new architectures for comparison\n",
        "- Tune optimization by setting `base_lr` and the like or simply training longer\n",
        "- Switching the solver type from `SGD` to an adaptive method like `AdaDelta` or `Adam`\n",
        "\n",
        "Feel free to explore these directions by editing the all-in-one example that follows.\n",
        "Look for \"`EDIT HERE`\" comments for suggested choice points.\n",
        "\n",
        "By default this defines a simple linear classifier as a baseline.\n",
        "\n",
        "In case your coffee hasn't kicked in and you'd like inspiration, try out\n",
        "\n",
        "1. Switch the nonlinearity from `ReLU` to `ELU` or a saturing nonlinearity like `Sigmoid`\n",
        "2. Stack more fully connected and nonlinear layers\n",
        "3. Search over learning rate 10x at a time (trying `0.1` and `0.001`)\n",
        "4. Switch the solver type to `Adam` (this adaptive solver type should be less sensitive to hyperparameters, but no guarantees...)\n",
        "5. Solve for longer by setting `niter` higher (to 500 or 1,000 for instance) to better show training differences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoLF6Db74s0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_net_path = 'mnist/custom_auto_train.prototxt'\n",
        "test_net_path = 'mnist/custom_auto_test.prototxt'\n",
        "solver_config_path = 'mnist/custom_auto_solver.prototxt'\n",
        "\n",
        "### define net\n",
        "def custom_net(lmdb, batch_size):\n",
        "    # define your own net!\n",
        "    n = caffe.NetSpec()\n",
        "    \n",
        "    # keep this data layer for all networks\n",
        "    n.data, n.label = L.Data(batch_size=batch_size, backend=P.Data.LMDB, source=lmdb,\n",
        "                             transform_param=dict(scale=1./255), ntop=2)\n",
        "    \n",
        "    # EDIT HERE to try different networks\n",
        "    # this single layer defines a simple linear classifier\n",
        "    # (in particular this defines a multiway logistic regression)\n",
        "    n.score =   L.InnerProduct(n.data, num_output=10, weight_filler=dict(type='xavier'))\n",
        "    \n",
        "    # EDIT HERE this is the LeNet variant we have already tried\n",
        "    # n.conv1 = L.Convolution(n.data, kernel_size=5, num_output=20, weight_filler=dict(type='xavier'))\n",
        "    # n.pool1 = L.Pooling(n.conv1, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
        "    # n.conv2 = L.Convolution(n.pool1, kernel_size=5, num_output=50, weight_filler=dict(type='xavier'))\n",
        "    # n.pool2 = L.Pooling(n.conv2, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
        "    # n.fc1 =   L.InnerProduct(n.pool2, num_output=500, weight_filler=dict(type='xavier'))\n",
        "    # EDIT HERE consider L.ELU or L.Sigmoid for the nonlinearity\n",
        "    # n.relu1 = L.ReLU(n.fc1, in_place=True)\n",
        "    # n.score =   L.InnerProduct(n.fc1, num_output=10, weight_filler=dict(type='xavier'))\n",
        "    \n",
        "    # keep this loss layer for all networks\n",
        "    n.loss =  L.SoftmaxWithLoss(n.score, n.label)\n",
        "    \n",
        "    return n.to_proto()\n",
        "\n",
        "with open(train_net_path, 'w') as f:\n",
        "    f.write(str(custom_net('mnist/mnist_train_lmdb', 64)))    \n",
        "with open(test_net_path, 'w') as f:\n",
        "    f.write(str(custom_net('mnist/mnist_test_lmdb', 100)))\n",
        "\n",
        "### define solver\n",
        "from caffe.proto import caffe_pb2\n",
        "s = caffe_pb2.SolverParameter()\n",
        "\n",
        "# Set a seed for reproducible experiments:\n",
        "# this controls for randomization in training.\n",
        "s.random_seed = 0xCAFFE\n",
        "\n",
        "# Specify locations of the train and (maybe) test networks.\n",
        "s.train_net = train_net_path\n",
        "s.test_net.append(test_net_path)\n",
        "s.test_interval = 500  # Test after every 500 training iterations.\n",
        "s.test_iter.append(100) # Test on 100 batches each time we test.\n",
        "\n",
        "s.max_iter = 10000     # no. of times to update the net (training iterations)\n",
        " \n",
        "# EDIT HERE to try different solvers\n",
        "# solver types include \"SGD\", \"Adam\", and \"Nesterov\" among others.\n",
        "s.type = \"SGD\"\n",
        "\n",
        "# Set the initial learning rate for SGD.\n",
        "s.base_lr = 0.01  # EDIT HERE to try different learning rates\n",
        "# Set momentum to accelerate learning by\n",
        "# taking weighted average of current and previous updates.\n",
        "s.momentum = 0.9\n",
        "# Set weight decay to regularize and prevent overfitting\n",
        "s.weight_decay = 5e-4\n",
        "\n",
        "# Set `lr_policy` to define how the learning rate changes during training.\n",
        "# This is the same policy as our default LeNet.\n",
        "s.lr_policy = 'inv'\n",
        "s.gamma = 0.0001\n",
        "s.power = 0.75\n",
        "# EDIT HERE to try the fixed rate (and compare with adaptive solvers)\n",
        "# `fixed` is the simplest policy that keeps the learning rate constant.\n",
        "# s.lr_policy = 'fixed'\n",
        "\n",
        "# Display the current training loss and accuracy every 1000 iterations.\n",
        "s.display = 1000\n",
        "\n",
        "# Snapshots are files used to store networks we've trained.\n",
        "# We'll snapshot every 5K iterations -- twice during training.\n",
        "s.snapshot = 5000\n",
        "s.snapshot_prefix = 'mnist/custom_net'\n",
        "\n",
        "# Train on the GPU\n",
        "s.solver_mode = caffe_pb2.SolverParameter.GPU\n",
        "\n",
        "# Write the solver to a temporary file and return its filename.\n",
        "with open(solver_config_path, 'w') as f:\n",
        "    f.write(str(s))\n",
        "\n",
        "### load the solver and create train and test nets\n",
        "solver = None  # ignore this workaround for lmdb data (can't instantiate two solvers on the same data)\n",
        "solver = caffe.get_solver(solver_config_path)\n",
        "\n",
        "### solve\n",
        "niter = 250  # EDIT HERE increase to train for longer\n",
        "test_interval = niter / 10\n",
        "# losses will also be stored in the log\n",
        "train_loss = zeros(niter)\n",
        "test_acc = zeros(int(np.ceil(niter / test_interval)))\n",
        "\n",
        "# the main solver loop\n",
        "for it in range(niter):\n",
        "    solver.step(1)  # SGD by Caffe\n",
        "    \n",
        "    # store the train loss\n",
        "    train_loss[it] = solver.net.blobs['loss'].data\n",
        "    \n",
        "    # run a full test every so often\n",
        "    # (Caffe can also do this for us and write to a log, but we show here\n",
        "    #  how to do it directly in Python, where more complicated things are easier.)\n",
        "    if it % test_interval == 0:\n",
        "        print('Iteration', it, 'testing...')\n",
        "        correct = 0\n",
        "        for test_it in range(100):\n",
        "            solver.test_nets[0].forward()\n",
        "            correct += sum(solver.test_nets[0].blobs['score'].data.argmax(1)\n",
        "                           == solver.test_nets[0].blobs['label'].data)\n",
        "        test_acc[int(it // test_interval)] = correct / 1e4\n",
        "\n",
        "_, ax1 = subplots()\n",
        "ax2 = ax1.twinx()\n",
        "ax1.plot(arange(niter), train_loss)\n",
        "ax2.plot(test_interval * arange(len(test_acc)), test_acc, 'r')\n",
        "ax1.set_xlabel('iteration')\n",
        "ax1.set_ylabel('train loss')\n",
        "ax2.set_ylabel('test accuracy')\n",
        "ax2.set_title('Custom Test Accuracy: {:.2f}'.format(test_acc[-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}